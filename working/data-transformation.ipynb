{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03433b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(os.path.pardir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e24fb150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-22 22:40:45,546 [INFO] : common - Successfully Loaded YAML file : config/config.yaml\n",
      "2026-01-22 22:40:45,550 [INFO] : io_types - Creating Directory artifacts/01_22_2026_22_40_43\n",
      "2026-01-22 22:40:45,576 [INFO] : ingestion - Dataset gta-crash already exists at data/raw/gta-crash\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataIngestionArtifact(names=['gta-crash'], path=Directory(path=PosixPath('data/raw')))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "from crash_detection.config import ConfigurationManager\n",
    "from crash_detection.components.data.ingestion import DataIngestionComponent\n",
    "\n",
    "config = ConfigurationManager()\n",
    "\n",
    "data_ingestion_config = config.get_data_ingestion_config()\n",
    "data_ingestion_artifact = DataIngestionComponent(data_ingestion_config)()\n",
    "data_ingestion_artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae067f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-22 22:40:45,589 [INFO] : common - Successfully Loaded YAML file : schemas/gta-crash.yaml\n",
      "2026-01-22 22:40:45,591 [INFO] : io_types - Creating Directory artifacts/01_22_2026_22_40_43/models\n",
      "2026-01-22 22:40:45,592 [INFO] : io_types - Creating Directory artifacts/01_22_2026_22_40_43/models/video-classifier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'video-classifier': ModelTrainingConfig(name='video-classifier', type='video-2d-classification', datasets={'gta-crash': DataSchema(name='gta-crash', train=BoxList(['train.csv']), train_image_folder='train_images/', test=BoxList(['test.csv']), test_image_folder='test_images/', columns=ConfigBox({'id': 'object', 'vid': 'int', 'fid': 'int', 'filename': 'object'}), categorical=BoxList(['vid', 'fid']), target='target', additional_properties=Box({}))}, transforms=DataTransformationConfig(indir=Directory(path=PosixPath('data/raw')), split=DataSplitConfig(type='skfold', ratio=0.8, n_splits=10), frames_per_clip=20, resize={'height': 224, 'width': 224}, normalize=False, grayscale=True, outdir=Directory(path=PosixPath('data/transformed'))), outdir=Directory(path=PosixPath('artifacts/01_22_2026_22_40_43/models/video-classifier')))}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_training_config = config.model_training_config(data_ingestion_artifact=data_ingestion_artifact)\n",
    "model_training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab2886e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/hello/world', '')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.split(\"/hello/world/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeea1512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Splitter \n",
    "from pandas import DataFrame\n",
    "from crash_detection.core.config_entity import DataTransformationConfig\n",
    "from crash_detection.core import Directory\n",
    "from crash_detection.errors import TransformationError\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from crash_detection.utils.common import save_csv\n",
    "from pandas.api.types import is_integer_dtype\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from typeguard import typechecked\n",
    "from crash_detection.core.config_entity import DataSchema\n",
    "from crash_detection import logger\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# @typechecked\n",
    "def split_dataset(\n",
    "    config: DataTransformationConfig,\n",
    "    data: pd.DataFrame,\n",
    "    schema: DataSchema,\n",
    "    filename: str,\n",
    "    outdir: Directory | None = None,\n",
    ") -> DataFrame:\n",
    "    split_config = config.split\n",
    "    dataname = schema.name\n",
    "\n",
    "    data[\"fold\"] = -1\n",
    "    _splits_ = [\"kfold\", \"skfold\"]\n",
    "\n",
    "    labels = schema.target\n",
    "    \n",
    "    if isinstance(labels, list):\n",
    "        if len(labels) > 1:\n",
    "            logger.error(\"Multi Label Stratified K-Fold is not implemented yet.\")\n",
    "            split_config.type = \"kfold\"\n",
    "        else:\n",
    "            labels = labels[0]\n",
    "            split_config.type = \"skfold\"\n",
    "\n",
    "    if split_config.type == \"kfold\":\n",
    "        splitter = KFold\n",
    "    elif split_config.type == \"skfold\":\n",
    "        splitter = StratifiedKFold\n",
    "\n",
    "    if hasattr(splitter, \"labels\"):\n",
    "        labels = split_config.labels\n",
    "\n",
    "    splitter = splitter(\n",
    "        split_config.n_splits,\n",
    "        shuffle=os.environ.get(\"PYTHONHASHSEED\", 0) != 0,\n",
    "        random_state= int(os.environ[\"PYTHONHASHSEED\"]) if hasattr(os.environ, \"PYTHONHASHSEED\") else None,\n",
    "    )\n",
    "    try:\n",
    "        le_columns = []\n",
    "        if labels:\n",
    "            if isinstance(labels, list):\n",
    "                for col in labels:\n",
    "                    if not is_integer_dtype(data[col]):\n",
    "                        logger.info(f\"Label Encoding the dataset column '{col}'\")\n",
    "                        data[f\"{col}_le\"] = LabelEncoder().fit_transform(data[col])\n",
    "                        col = f\"{col}_le\"\n",
    "                    le_columns.append(col)\n",
    "\n",
    "            elif isinstance(labels, str):\n",
    "                le_columns = labels\n",
    "                if not is_integer_dtype(data[labels]):\n",
    "                    le_columns = f\"{labels}_le\"\n",
    "                    data[le_columns] = LabelEncoder().fit_transform(data[labels])\n",
    "\n",
    "            else:\n",
    "                e = TransformationError(\n",
    "                    \"Labels are neither str or list[str]\",\n",
    "                    dataname=dataname,\n",
    "                    file_name=filename,\n",
    "                )\n",
    "                logger.error(e)\n",
    "                raise e\n",
    "\n",
    "            logger.info(\n",
    "                f\"Folding '{dataname}.{filename}' into {split_config.n_splits} using {split_config.type} on column(s) {le_columns}\"\n",
    "            )\n",
    "\n",
    "            for fold, (_, test_index) in enumerate(\n",
    "                splitter.split(data, data[le_columns])\n",
    "            ):\n",
    "                data.loc[test_index, \"fold\"] = fold\n",
    "\n",
    "            if isinstance(le_columns, str):\n",
    "                le_columns = le_columns if le_columns.endswith(\"_le\") else None\n",
    "            elif isinstance(le_columns, list):\n",
    "                le_columns = [col for col in le_columns if col.endswith(\"_le\")]\n",
    "\n",
    "            if le_columns:\n",
    "                data = data.drop(le_columns, axis=1)\n",
    "\n",
    "    except Exception as e:\n",
    "        e = TransformationError(\n",
    "            f\"Labels are not given to use {split_config.type} folding technique. \\nUsing Regular KFold\",\n",
    "            dataname=dataname,\n",
    "            file_name=filename,\n",
    "        )\n",
    "        logger.error(e)\n",
    "\n",
    "        if le_columns:\n",
    "            if isinstance(le_columns, list):\n",
    "                data = data.drop(\n",
    "                    [col for col in le_columns if col.endswith(\"_le\")], axis=1\n",
    "                )\n",
    "            elif le_columns.endswith(\"_le\"):\n",
    "                data = data.drop(le_columns, axis=1)\n",
    "\n",
    "        splitter = KFold(\n",
    "            split_config.n_splits,\n",
    "            shuffle=os.environ.get(\"PYTHONHASHSEED\", 0) != 0,\n",
    "            random_state=int(os.environ.get(\"PYTHONHASHSEED\", 1234)),\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"Folding '{dataname}.{filename}' into {split_config.n_splits} using kfold\"\n",
    "        )\n",
    "\n",
    "        for fold, (_, test_index) in enumerate(splitter.split(data)):\n",
    "            data.loc[test_index, \"fold\"] = fold\n",
    "\n",
    "    if outdir:\n",
    "        save_csv(data, outdir // f\"folded_{dataname}\" / filename)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f883f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-22 22:43:33,269 [INFO] : common - Successfully loaded CSV file : data/raw/gta-crash/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parallel Cache Input:   0%|          | 5/11381 [00:00<02:53, 65.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-22 22:43:33,670 [INFO] : 976515572 - Folding 'gta-crash.train.csv' into 10 using skfold on column(s) target\n",
      "2026-01-22 22:43:33,761 [INFO] : common - Successfully saved CSV file : data/transformed/gta-crash/train.csv\n",
      "2026-01-22 22:43:33,767 [INFO] : common - Successfully saved CSV file : data/transformed/gta-crash/valid.csv\n",
      "2026-01-22 22:43:33,792 [INFO] : common - Successfully loaded CSV file : data/raw/gta-crash/test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parallel Cache Input:   5%|â–         | 5/111 [00:00<00:01, 83.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-22 22:43:33,865 [INFO] : common - Successfully saved CSV file : data/transformed/gta-crash/test.csv\n",
      "2026-01-22 22:43:33,881 [INFO] : common - Successfully saved CSV file : data/transformed/train.csv\n",
      "2026-01-22 22:43:33,887 [INFO] : common - Successfully saved CSV file : data/transformed/valid.csv\n",
      "2026-01-22 22:43:33,888 [INFO] : common - Successfully saved CSV file : data/transformed/test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from crash_detection.core.config_entity import DataTransformationConfig, DataSchema\n",
    "from crash_detection.core.artifact_entity import DataTransformationArtifact\n",
    "from crash_detection.utils.common import load_csv, save_csv\n",
    "from crash_detection import logger\n",
    "from crash_detection.core import Directory\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typeguard import typechecked\n",
    "from tqdm import tqdm\n",
    "from functools import partial   \n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class DataTransformationComponent:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    # @typechecked\n",
    "    def _cache_video(self, vid : int, indir : Path, outdir : Directory) -> list[np.ndarray]:\n",
    "        cache_file = outdir / f\"{str(vid).zfill(5)}.npy\"\n",
    "        video_tensor = np.zeros((224, 224, 20), dtype=np.uint8)\n",
    "        for i in range(20):\n",
    "            path = indir / f\"{str(vid).zfill(5)}_{str(i).zfill(2)}.jpg\"\n",
    "\n",
    "            image = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
    "            image = cv2.resize(image, (224, 224))\n",
    "            video_tensor[:, :, i] = image\n",
    "        \n",
    "        np.save(cache_file, video_tensor)\n",
    "\n",
    "    def transform(self, data : pd.DataFrame, indir: Directory, outdir: Directory) -> bool:\n",
    "\n",
    "        vid_list = data[\"vid\"].unique().tolist()\n",
    "\n",
    "        cache_func = partial(self._cache_video, indir=indir, outdir=outdir)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=4) as pool:\n",
    "            list(\n",
    "                tqdm(\n",
    "                    pool.map(cache_func, vid_list[:5]),\n",
    "                    desc=\"Parallel Cache Input\",\n",
    "                    total=len(vid_list),\n",
    "                )\n",
    "            )\n",
    "        return True\n",
    "    \n",
    "    def __call__(self, datasets: dict[str, DataSchema]) -> None:\n",
    "        data_path = self.config.indir\n",
    "        if data_path.exists():\n",
    "            train_datas = []\n",
    "            valid_datas = []\n",
    "            test_datas = []\n",
    "        else:\n",
    "            e = FileNotFoundError(f\"Data path {data_path} does not exist.\")\n",
    "            logger.error(e)\n",
    "            raise e\n",
    "\n",
    "        for name, schema in datasets.items():\n",
    "            for split in ['train', 'test']:\n",
    "                for filename in eval(f\"schema.{split}\"):\n",
    "                    data = load_csv(self.config.indir / name / filename)\n",
    "                    data = data[['vid', schema.target]].drop_duplicates(ignore_index=True)\n",
    "                    self.transform(\n",
    "                        data, \n",
    "                        self.config.indir // name // getattr(schema, f\"{split}_image_folder\"), \n",
    "                        self.config.outdir // name // getattr(schema, f\"{split}_image_folder\")\n",
    "                    )\n",
    "\n",
    "                    data['filename'] = data.apply(lambda row: self.config.outdir / name / getattr(schema, f\"{split}_image_folder\") / f\"{str(row['vid']).zfill(5)}.npy\", axis=1)\n",
    "                    \n",
    "\n",
    "                    if split == 'train':\n",
    "                        data = split_dataset(config=self.config, data=data, schema=schema, filename=filename, outdir=None)\n",
    "                        train_folds = int(self.config.split.n_splits * self.config.split.ratio)\n",
    "\n",
    "                        train_data = data[data['fold'] < train_folds].reset_index(drop=True)[['vid', schema.target, 'filename']]\n",
    "\n",
    "                        save_csv(self.config.outdir / name / f\"{split}.csv\", data=data)\n",
    "\n",
    "                        valid_data = data[data['fold'] >= train_folds].reset_index(drop=True)[['vid', schema.target, 'filename']]\n",
    "\n",
    "                        train_datas.append(train_data)\n",
    "                        if not valid_data.empty:\n",
    "                            valid_datas.append(valid_data)\n",
    "                            save_csv(self.config.outdir / name / \"valid.csv\", data=valid_data)\n",
    "                    else:\n",
    "                        test_datas.append(data)\n",
    "                        save_csv(self.config.outdir / name / f\"{split}.csv\", data=data)\n",
    "\n",
    "        train_data = pd.concat(train_datas).reset_index(drop=True)\n",
    "        valid_data = pd.concat(valid_datas).reset_index(drop=True)\n",
    "        test_data = pd.concat(test_datas).reset_index(drop=True)\n",
    "\n",
    "        save_csv(self.config.outdir / \"train.csv\", data=train_data)\n",
    "        save_csv(self.config.outdir / \"valid.csv\", data=valid_data)\n",
    "        save_csv(self.config.outdir / \"test.csv\", data=test_data)\n",
    "        \n",
    "        return DataTransformationArtifact(\n",
    "            path=self.config.outdir,\n",
    "            train_file_path=self.config.outdir / \"train.csv\",\n",
    "            valid_file_path=self.config.outdir / \"valid.csv\",\n",
    "            test_file_path=self.config.outdir / \"test.csv\",\n",
    "        )\n",
    "\n",
    "\n",
    "for model, config in model_training_config.items():\n",
    "    data_transformation_component = DataTransformationComponent(config=config.transforms)\n",
    "    data_transformation_component(config.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a96f9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'42'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"PYTHONHASHSEED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7006604c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crash-detection-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

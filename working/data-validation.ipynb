{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24105c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/Users/morizin/Documents/Code/crash-detection-project\")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f7f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE_PATH = \"config/config.yaml\"\n",
    "SCHEMA_DIR = \"schemas/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16918123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crash_detection.config.config_entity import DataValidataionConfig, DataSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab47677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crash_detection.config import ConfigurationManager\n",
    "from crash_detection.components.data.ingestion import DataIngestionComponent\n",
    "\n",
    "config = ConfigurationManager(config_path=CONFIG_FILE_PATH)\n",
    "data_ingestion_config = config.get_data_ingestion_config()\n",
    "\n",
    "print(data_ingestion_config)\n",
    "\n",
    "data_ingestion_artifact =  DataIngestionComponent(data_ingestion_config=data_ingestion_config)()\n",
    "\n",
    "data_validation_config = config.get_data_validation_config(\n",
    "    data_ingestion_artifact=data_ingestion_artifact\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4bb2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from crash_detection import logger\n",
    "from tqdm import tqdm\n",
    "from crash_detection.config.config_entity import DataValidataionConfig\n",
    "from crash_detection.config.artifact_entity import DataValidationArtifact\n",
    "from crash_detection.utils.common import save_json, save_csv\n",
    "tqdm.pandas()\n",
    "\n",
    "def load_csv(file_path: Path | str) -> pd.DataFrame:\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "class DataValidationComponent:\n",
    "    def __init__(self, config: DataValidataionConfig):\n",
    "        self.config = config\n",
    "        tree = lambda: defaultdict(tree)\n",
    "        self.status = tree()\n",
    "        self.status['validation_status'] = True\n",
    "\n",
    "    def validate(self) -> DataValidationArtifact:\n",
    "        # Schema validation logic goes here\n",
    "        schemas = self.config.schemas\n",
    "        for schema_name, schema in schemas.items():\n",
    "            logger.info(f\"Validating dataset: {schema_name}\")\n",
    "            train_files = []\n",
    "            test_files = []\n",
    "\n",
    "            for file in schema.train + schema.test:\n",
    "                basename = str(os.path.basename(file))\n",
    "                logger.info(f\"Validating file: {basename}\")\n",
    "                if os.path.exists(file):\n",
    "                    df = load_csv(file)\n",
    "                elif not os.path.exists(os.path.join(schema.path, file)):\n",
    "                    df = load_csv(os.path.join(schema.path, file))\n",
    "                else:\n",
    "                    logger.error(f\"File {file} not found for dataset {schema_name}\")\n",
    "                    self.status[schema_name][basename] = {\n",
    "                        'schema_validation': False,\n",
    "                        \"additional_invalid_schema_column\": [],\n",
    "                        'missing_column': True,\n",
    "                        \"additional_missing_column\": [],\n",
    "                        'missing_image': False\n",
    "                    }\n",
    "                    self.status['validation_status'] = False\n",
    "                    continue\n",
    "\n",
    "                logger.info(f\"Loaded data shape: {df.shape}\")\n",
    "\n",
    "                self.status[schema_name][basename] = {\n",
    "                    'schema_validation': True,\n",
    "                    \"additional_invalid_schema_column\": [],\n",
    "                    'missing_column': False,\n",
    "                    \"additional_missing_column\": [],\n",
    "                    'missing_image': False\n",
    "                }\n",
    "                for col, dtype in schema.columns.items():\n",
    "                    if col in df.columns:\n",
    "                        if not pd.api.types.is_dtype_equal(df[col].dtype, dtype):\n",
    "                            logger.error(f\"Column {col} has dtype {df[col].dtype}, expected {dtype}\")\n",
    "                            self.status[schema_name][basename]['schema_validation'] = False\n",
    "                            self.status['validation_status'] = False\n",
    "\n",
    "                    else:\n",
    "                        logger.error(f\"Column {col} is missing in the dataset\")\n",
    "                        self.status[schema_name][basename]['missing_column'] = True\n",
    "                        self.status['validation_status'] = False\n",
    "                        \n",
    "                for col, dtype in schema.additional_properties.items():\n",
    "                    if col in df.columns:\n",
    "                        if not pd.api.types.is_dtype_equal(df[col].dtype, dtype):\n",
    "                            logger.warning(f\"Additional column {col} has dtype {df[col].dtype}, expected {dtype}\")\n",
    "                            self.status[schema_name][basename]['additional_invalid_schema_column'].append(col)\n",
    "                            self.status['validation_status'] = False\n",
    "\n",
    "                    else:\n",
    "                        logger.warning(f\"Additional column {col} is missing in the dataset\")\n",
    "                        self.status[schema_name][basename]['additional_missing_column'].append(col)\n",
    "                \n",
    "                # Remove undefined columns\n",
    "                for col in self.status[schema_name][basename]['additional_invalid_schema_column']:\n",
    "                    logger.warning(f\"Dropping additional column {col} due to invalid schema\")\n",
    "                    df.drop(columns=[col], inplace=True)\n",
    "\n",
    "                for col in df.columns:\n",
    "                    if col not in schema.columns and col not in schema.additional_properties and col != schema.target:\n",
    "                        logger.warning(f\"Dropping column {col} since not defined in the schema\")\n",
    "                        df.drop(columns=[col], inplace=True)\n",
    "                        self.status[schema_name][basename]['additional_missing_column'].append(col)\n",
    "\n",
    "                train = True if file in schema.train else False\n",
    "\n",
    "                image_folder = schema.train_image_folder if train else schema.test_image_folder\n",
    "\n",
    "                if not image_folder.exists():\n",
    "                    image_folder = schema.path // image_folder\n",
    "\n",
    "                df['filename'] = df['filename'].progress_apply(lambda x: str(image_folder / x))\n",
    "\n",
    "                missing_images = df['filename'].progress_apply(lambda x: not os.path.exists(x))\n",
    "\n",
    "                if missing_images.any():\n",
    "                    logger.error(f\"{missing_images.sum()} Missing images found in dataset {schema_name}, file { os.path.basename(file)}\")\n",
    "                    valid_df = df[~missing_images].reset_index(drop=True)\n",
    "                    invalid_df = df[missing_images].reset_index(drop=True)\n",
    "                    self.status[schema_name][basename]['missing_image'] = True\n",
    "                    self.status['validation_status'] = False\n",
    "                else:\n",
    "                    valid_df = df\n",
    "                    invalid_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "                save_csv(path = self.config.outdir // schema_name // 'valid' / basename, data = valid_df)\n",
    "\n",
    "                if train:\n",
    "                    schema.train = [f for f in schema.train if f != file]\n",
    "                logger.info(f\"Saved validated data to {self.config.outdir // schema_name // 'valid' / basename}\")\n",
    "\n",
    "                if missing_images.any():\n",
    "                    save_csv(path = self.config.outdir // schema_name // 'invalid' / basename, data = invalid_df)\n",
    "                    logger.info(f\"Saved invalid data to {self.config.outdir // schema_name // 'invalid' / basename}\")\n",
    "\n",
    "                eval(f\"{\"train\" if train else \"test\"}_files\").append(self.config.outdir // schema_name // 'valid' / basename)\n",
    "                \n",
    "            schema.train = train_files\n",
    "            schema.test = test_files\n",
    "\n",
    "        save_json(self.config.report_path, self.status)\n",
    "        logger.info(f\"Saved validation report to {self.config.report_path}\")\n",
    "        \n",
    "        return DataValidationArtifact(\n",
    "            report_path=self.config.report_path,\n",
    "            image_dir=self.config.indir,\n",
    "            valid_data_dir=self.config.outdir // 'valid',\n",
    "            invalid_data_dir=self.config.outdir // 'invalid',\n",
    "            schemas = schemas,\n",
    "            is_validated=self.status['validation_status']\n",
    "        )\n",
    "    \n",
    "    def __call__(self) -> DataValidationArtifact:\n",
    "        return self.validate()\n",
    "            \n",
    "data_validation = DataValidationComponent(config=data_validation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2c9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_validation_artifact = data_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee24182",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crash-detection-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

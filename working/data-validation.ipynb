{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24105c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/Users/morizin/Documents/Code/crash-detection-project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f7f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE_PATH = \"config/config.yaml\"\n",
    "SCHEMA_DIR = \"schemas/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16918123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, model_validator\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from box import ConfigBox\n",
    "import yaml\n",
    "from glob import glob   \n",
    "from src.crash_detection import logger\n",
    "from src.crash_detection.utils.common import load_yaml\n",
    "\n",
    "class DataSchema(BaseModel):\n",
    "    name: str\n",
    "    path: Path | str\n",
    "\n",
    "    train: Optional[str] = None\n",
    "    train_image_folder: Optional[str] = None \n",
    "    test: Optional[str] = None\n",
    "    test_image_folder: Optional[str] = None\n",
    "\n",
    "    columns: Optional[dict[str, str]] = None\n",
    "    categorical: Optional[list[str]] = None\n",
    "    target: Optional[str] = None\n",
    "    additional_properties: Optional[dict[str, str]] = None\n",
    "\n",
    "\n",
    "    def model_post_init(self, __context__):\n",
    "        file_path = os.path.join(SCHEMA_DIR, f\"{self.name}.yaml\")\n",
    "        if os.path.exists(file_path):\n",
    "            content = load_yaml(file_path)\n",
    "\n",
    "            self.train = content[\"train\"]\n",
    "            self.train_image_folder = content[\"train_image_folder\"]\n",
    "            self.test = content[\"test\"]\n",
    "            self.test_image_folder = content[\"test_image_folder\"]\n",
    "\n",
    "            self.columns = content[\"columns\"]\n",
    "            self.categorical = content.get(\"categorical\", [])\n",
    "            self.additional_properties = content.get(\"additional_properties\", {})\n",
    "            self.target = content[\"target\"]\n",
    "            return self\n",
    "        else:\n",
    "            e = f\"Schema of dataset '{self.name}' not found\"\n",
    "            logger.error(e)\n",
    "            raise Exception(e)\n",
    "\n",
    "class DataValidataionConfig(BaseModel):\n",
    "    report_name: str\n",
    "    indir: Path | str\n",
    "    outdir: Path | str\n",
    "    pixel_histogram: bool\n",
    "    statistics: bool\n",
    "    kl_divergence: bool\n",
    "    schemas: dict[str, DataSchema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b3572b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.crash_detection.utils.common import load_yaml\n",
    "from src.crash_detection.constants import SCHEMA_DIR, DATA_DIRECTORY_NAME, REPORT_NAME\n",
    "\n",
    "class ConfigutationManager:\n",
    "    def __init__(self, config_file_path=CONFIG_FILE_PATH):\n",
    "        self.config_file = load_yaml(config_file_path)\n",
    "        self.artifact_path = self.config_file[\"artifact-path\"]\n",
    "\n",
    "\n",
    "    def get_data_validation_config(self):\n",
    "        data_schemes = {}\n",
    "        for d_name in (self.config_file.data_sources):\n",
    "            data_schemes[d_name] = DataSchema(name=d_name, path = os.path.join(self.artifact_path, DATA_DIRECTORY_NAME, d_name))\n",
    "        # data_validation_config = self.config_file[\"data_validation\"]\n",
    "        return DataValidataionConfig(\n",
    "            report_name = os.path.join(self.artifact_path, DATA_DIRECTORY_NAME, f\"{REPORT_NAME}.yaml\"),\n",
    "            indir = os.path.join(self.artifact_path, DATA_DIRECTORY_NAME),\n",
    "            outdir = os.path.join(self.artifact_path, REPORT_NAME),\n",
    "            pixel_histogram = False,\n",
    "            statistics = False,\n",
    "            kl_divergence = False,\n",
    "            schemas = data_schemes\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82187348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-22 02:30:22,237 [INFO] : common - Successfully Loaded YAML file : config/config.yaml\n",
      "2026-01-22 02:30:22,245 [INFO] : common - Successfully Loaded YAML file : schemas/gta-crash.yaml\n"
     ]
    }
   ],
   "source": [
    "cfg = ConfigutationManager()\n",
    "data_validation_config = cfg.get_data_validation_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4bb2d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1295452104.py, line 19)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mif dtype ==\u001b[39m\n                ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_csv(file_path: Path | str) -> pd.DataFrame:\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "class DataValidationComponent:\n",
    "    def __init__(self, config: DataValidataionConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def validate(self):\n",
    "        # Schema validation logic goes here\n",
    "        for schema_name, schema in self.config.schemas.items():\n",
    "            logger.info(f\"Validating dataset: {schema_name}\")\n",
    "\n",
    "            df = load_csv(os.path.join(schema.path, schema.train))\n",
    "            logger.info(f\"Loaded data shape: {df.shape}\")\n",
    "            for col, dtype in schema.columns.items():\n",
    "                if col in df.columns:\n",
    "                    if not pd.api.types.is_dtype_equal(df[col].dtype, dtype):\n",
    "                        logger.warning(f\"Column {col} has dtype {df[col].dtype}, expected {dtype}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Column {col} is missing in the dataset\")\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "data_validation = DataValidationComponent(config=data_validation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee24182",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crash-detection-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
